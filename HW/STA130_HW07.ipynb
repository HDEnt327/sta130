{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA130 Homework 07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT Chat log links:\n",
    "\n",
    "Clarification and review on indicator variables:\n",
    "MLR and Overfitting:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### 1. Difference between SLR and MLR\n",
    "\n",
    "Simple Linear Regression, or SLR, is a statistical model that attemps to estimate the relationship between a predicator (independent) variable and a outcome (dependent) variable. SLR models a one-on-one relationship, where an input $X$ gives an output $Y$. An indicator variable might also be involved in SLR model, representing a difference between groups under a similar $X \\sim Y$ relationship.\n",
    "\n",
    "An example linear form of a SLR model can be:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "Where $y$ is the outcome and $x$ is the predictor variable, and $\\beta_0$ and $\\beta_1$ are the coefficients.\n",
    "\n",
    "Multiple Linear Regression, or MLR, is also a statistical model that estimates the relationship between predicator and outcome variables, except in MLR, instead of a one-on-one relationship, multiple predicator variables may be used to predict one outcome variable. \n",
    "\n",
    "An example linear form of a MLR model can be:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon$$\n",
    "\n",
    "Where $x_1$ $x_2$ $x_3$ are each different predictor variables.\n",
    "\n",
    "The benefits of MLR over SLR are that is it able to model the effect of multiple predictor variables on the outcome variable simultaneously. Often time when trying to model these relationships, there are way more than one variables that effect the outcome simultaneously. While SLR can only model the relationship between one variable and another, MLR can model much more complex relationships through interactions, polynomial terms, or even transformations to model non-linear and non-additive relationships. (We will explain what additive and synergistic relationships are soon)\n",
    "\n",
    "### 2. Difference between continuous variable and indicator variable in an SLR model\n",
    "\n",
    "The difference between a continuous variable and an indicator variable is that, with a continuous variable, we are modelling how much variation there is in $Y$ in comparison to how much variation there is in $X$. While for an indivator variable, we are only looking at how $Y$ changes (or the difference in $Y$) between different indicator groups.\n",
    "\n",
    "The linear form of a SLR model with a continuous variable looks something like:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\epsilon$$\n",
    "\n",
    "Where $\\beta_1$ is the coefficient that measures the defines the magnitude of significance changes in $X$ has on $Y$ (or just the slope), and $\\beta_0$ is the y-intercept.\n",
    "\n",
    "The linear form of a SLR model with an indicator variable look something like:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 1_\\text{[x=\"A\"]} + \\epsilon$$\n",
    "\n",
    "Where $\\beta_0$ is the outcome variable for when $x$ is \"A\", and $\\beta_1$ is the difference between the outcome variable for when $x$ is \"A\" and when $x$ is \"B\".\n",
    "\n",
    "### 3. When an indicator variable is introduced alongside a continuous variable to create a MLR model\n",
    "\n",
    "When we have an indicator variable introduced alongside a continuous variable in an SLR model to create an MLR model, we are essentially creating parallel linear functions, where one of the functions describe the relationship between the continuous predictor and the outcome under one situation, and the other function describes the relationship under another situation.\n",
    "\n",
    "The linear form of a SLR model looks like:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\epsilon$$\n",
    "\n",
    "Where $\\beta_1$ is the coefficient that measures the defines the magnitude of significance changes in $X$ has on $Y$ (or just the slope), and $\\beta_0$ is the y-intercept.\n",
    "\n",
    "When introducing a indicator variable along with it, the linear form looks like:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 1_\\text{[x=\"A\"]} + \\epsilon$$\n",
    "\n",
    "Where the $\\beta_2$ coefficient represents the difference between the intercepts of the parallel lines. As a result, we have two parallel lines with a slope of $\\beta_1$, one with a y-intercept of $\\beta_0$, the other with a y-intercept of $\\beta_2$.\n",
    "\n",
    "### 4. The effect of adding an interaction between a continuous and an indicator variable in MLR models\n",
    "\n",
    "\n",
    "\n",
    "### 5. The behavior of a MLR model based only on indicator variables derived from a non-binary categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "To explain this seemingly contradictory phenomenon of having very low p-value for many of the coefficients in an MLR model, but also having a low R-squared value, we need to first revisit and understand what each of them are really representing.\n",
    "\n",
    "We've explored before from HW06 last week that the R-squared value for a fitted regression model measures how accurately the model represents the real relationship between $X$ and $Y$, and it specifically refers to the proportion of variation in $Y$ that is explained by the variation in $X$. Note that **the R-squared value measures the accuracy of the model as a whole**, looking at how well the model predicts the outcome variable based on predictor variable input.\n",
    "\n",
    "We've also briefly talked about how we interpret the fitted results from a regression model specification, how a lower p-value for a specific coefficient means that this predictor variable is likely to have an effect on the outcome variable. We know this, because a low p-value provides evidence against a null hypothesis, in the context of linear regression, the null hypothesis is that the predictor variable has no effect on the outcome variable. Therefore, the low p-value shows evidence against this. \n",
    "\n",
    "Now, note how **the p-value only looks at whether or not a specific variable is related (or has an effect on) the outcome variable**, and not the entire model as a whole or any other variables. In other words, **a low p-value from a specific predictor variable does not imply that it explains a substantial portion of the variability in the outcome variable.**\n",
    "\n",
    "Furthermore, although having a low p-value may suggest significant relationship, it's real siginificance in the model as a whole may not be as prominent. This can most likely be explained looking at the coefficients for the predictor variables. If the magnitude of the coefficients for the predictor variable is low, although having a significant relationship with the outcome, the overall significance of this variable in the model is low, since any variations in this predictor will end up with very little variations in the outcome in the end. **Therefore, those predictors who have a low/insignificant coefficient might not account for the accuracy of the model overall.**\n",
    "\n",
    "Looking at the given example, we see that many of the \"Sp. Def\" predictor variables have a relatively insignificant coefficient, many of which are less than 1. This means that these predictors do not contribute as much to the model as a whole, and therefore their low p-values do not speak much for the accuracy of the model.\n",
    "\n",
    "Going back a little, the model specification itself also might cause this type of phenomenon. With an existing specification, if we see that many of our predictors have a low p-value yet the R-squared for the model is low, we might be missing something in our specification, that is actually needed to be included in the consideration of the model for the model to fit the data better. **In other words, there may be other factors that contribute to the variability in $Y$ that is not included in the current model specification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "These five cells of code are mainly illustrating the concept of **Overfitting**, which is when we try to create a model that overly captures the characteristics of the training data and fails to generalize the relationship between the predictor and outcome variables. **Overfitting** is commonly caused by an overly complex model, that essentially tries to \"memorize\" the training data instead of modelling or generalizing it. As a result, when we try to use the model to explain the variation of the outcomes in the testing data, we get a incredibly low result.\n",
    "\n",
    "The first code block uses the `train_test_split` method from `sklearn.model_selection` to create a testing and training dataset. The `pokeaman_train` part will be used to fit the MLR model specification, and the `pokeaman_test` part will be used to test the fitted model.\n",
    "\n",
    "Then, a MLR model specification is created a fitted, with the predictor variables being `Attack` and `Defense` and the outcome variable being `HP`. This MLR model specification solely looks at the two predictors additively, meaning that one predictor's influence on `HP` is not affected by the other predictor, so there is no interaction between the two.\n",
    "\n",
    "The linear form of this specification is:\n",
    "\n",
    "$$Y=\\beta_0 + \\beta_1 \\cdot \\text{Attack} + \\beta_2 \\cdot \\text{Defense} + \\epsilon$$\n",
    "\n",
    "In the results, we indeed see that both `Attack` and `Defense` have a significant relationship with `HP`, however our model does not explain much of the variability in `HP`, with a R-squared value of only 0.148. Note that this R-squared value is calculated by calculating the squared correlation coefficient between the predicted values for `HP` from the model and the values of `HP` from the observation (the `pokeaman_train` dataset).\n",
    "\n",
    "To look at whether or not this model really represents the relationship between the outcome and predicator variables, we can take a testing dataset and see the R-squared value for it, and this is exactly what the `pokeaman_test` data is for. As a result, surprisingly, we see a similar yet better resulting R-squared of around 0.212.\n",
    "\n",
    "But our R-squared value is still quite low, it doesn't represent the relationship well enough, so we can make some changes to our specification so that it can better predict the outcomes. In the next code block, we create a new MLR model specification that is much more complex, with interactions between `Attack`, `Defense`, `Speed`, and `Legendary`, plus interactions between `Sp. Def` and `Sp. Atk`. This ends up with a really complex linear form that includes all possible combinations between the four interacting predictors and the two `Sp.` interacting predictors that is probably too much to type here in the notebook...\n",
    "\n",
    "After fitting this MLR model, we end up with a R-squared of 0.467, which is much higher than before. But don't forget that this is tested on the provided dataset, to see if the model really predicts the outcome well, we need to use the testing dataset to test the model. As a result, we end up with a `Out of sample R-squared` of only 0.002.\n",
    "\n",
    "As we've explained before, this is one of the indicators of a severely overfitted regression model, where the fitted model fails to generalize the relationship between outcome and predicator values, frequently trying to capture those very intricate characteristics from the training dataset, causing it fail to predict values from the testing dataset. Overfitting generally happens when the the model specification is overly complex relative to the available data used to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "We've explored how an overly complex MLR model specification can lead to overfitting when the complexity relatively exceeds the available data. As a result, although the model fits the fitted data well, when we use a new piece of data to test the model, it fails to accurately predict the outcomes. We learn that by using a in-sample and out-of-sample test to find the R-squared value, we can see whether or not overfitting has occured, as observed by a great drop in the R-squared value.\n",
    "\n",
    "Now, is there a way we could identify these possible overfittings without having such a comparison between in-sample and out-sample tests. The ultimate goal of such regression models is being able to accurately predict the outcome variables while maintaining good generalizability, meaning that the model is able to generally give good predictions to any given data.\n",
    "\n",
    "One thing that affects the generalizability of model fits is multicollinearity, it is when two or more predictor variables in the model are highly correlated to each other. When predictors are correlated, the regression algorithm will struggle to identify unique contributions each predicator makes to the outcome, as they carry redundant information. This causes the coefficients for these predictor variables to be highly unstable, and therefore fail to generalize as these coefficients are reflecting the noise in the data and the redundancy of the predictors, and not the true relationships between the predictors and the outcomes.\n",
    "\n",
    "One way to measure the multicollinearity that is present in a model fit is the **condition number** of a **design matrix** of the model specification. The design matrix is a matrix representation of the predictor variables in a regression model, where the rows represent the observations (or data points) and the columns represent the predictors (including any transformations and/or interactions). The representation of a regression model specification in this format simplifies the computation for both fitting the model, and doing diagnostics such as calculating our **condition number**.\n",
    "\n",
    "Now, like we've said, the condition number is a measure of multicollinearity present in a model fit, where a very large condition number suggests a large degree of multicollinearity, and therefore suggests that the fitted model is overfitted and questions the generalizability of the model.\n",
    "\n",
    "We can actually see this happening from the preivous example in `model4`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.672  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A condition number of 12 and then 15 zeros, this is a very large condition number! Therefore suggesting that this model has high multicollinearity, and thus we see the bad generalizability when testing the model with the testing dataset.\n",
    "\n",
    "# NOT DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "In this first improved version of the MLR model specification based on `model3`, we can see that in `model5`, it maintains the simplicity of `model3` in a sense that there are not many interactions leading to high complexity, and predictor relationships are purely additive. Like we've previously addressed, when the complexity of the model specification (relatively) exceeds the amount of data in the dataset, overfitting is likely to occur. What makes `model5` different from `model3` however is that `model5` presents adds on more predictors, like in `model4`, including `speed`, `Sp. Def`, and `Sp. Atk`, as well as categorical variable predictors, including `Generation`, `Type 1` and `Type 2`, These categorical variables will be automatically expanded into dummy variables.\n",
    "\n",
    "Although a lot more predictors have been added to the specification, like we've just said, there are not interactions that lead to high complexity, and therefore the model is kept simple, maintaining generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>286.476</td> <th>  Durbin-Watson:     </th> <td>   1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5187.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.807</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>19.725</td>  <th>  Cond. No.          </th> <td>9.21e+03</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 286.476 & \\textbf{  Durbin-Watson:     } &    1.917  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5187.327  \\\\\n",
       "\\textbf{Skew:}          &   2.807 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  19.725 & \\textbf{  Cond. No.          } & 9.21e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following improved version of the MLR model specification is based on `model5`, and also includes a number of changes that make the model have better generalizability. For example, in the base formula, the main continuous predictors now only include `Attack`, `Speed`, `Sp. Def`, and `Sp. Atk` ad additive predictors. `Defense` and `Legendary` were excluded as they may have contributed to complex interaction terms and multicollinearity.\n",
    "\n",
    "Furthermore, `model6` has added binary indicator variables, such as `I(Q(\"Type 1\")==\"Normal\")`, `I(Q(\"Type 1\")==\"Water\")`, `I(Generation==2)`, `I(Generation==5)`. By using binary indicator variables, we are looking at specific indicators and groups that most likely are relevant to variability in the outcome variable, and focusing on them to improve interpretability and generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>271.290</td> <th>  Durbin-Watson:     </th> <td>   1.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4238.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.651</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>18.040</td>  <th>  Cond. No.          </th> <td>    618.</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 271.290 & \\textbf{  Durbin-Watson:     } &    1.999  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 4238.692  \\\\\n",
       "\\textbf{Skew:}          &   2.651 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  18.040 & \\textbf{  Cond. No.          } &     618.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have `model7` that is based on `model6`, which its main difference is introducing interactions between variables. Contrary to the specifications earlier that also introduced interactions, which actually caused lots of unncessary complexity leading to overfitting, `model7` focuses interactions onto specific, continuous predictors: `Attack`, `Speed`, `Sp. Def`, `Sp. Atk`, that are likely to have some natural relationship between one another.\n",
    "\n",
    "The earlier models (such as `model4`) lead to overfitting with the introduction of interactions because the interacting variables were introduced without regard to their relevance, often mixing continuous and categorical predictors, or including very high-order terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a very large condition number though, we can do some centering and scaling to reduce the multicollinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>    15.4</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } &     15.4  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our condition number is only 15.4, which suggests small problems related to multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
