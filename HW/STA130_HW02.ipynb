{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA130 Week 02 Homework (Prelecture)\n",
    "\n",
    "## Question 1\n",
    "\n",
    "---\n",
    "\n",
    "### Conversation with ChatGPT\n",
    "\n",
    "**Me:** I have a python implementation of a simulation of the Monte Hall problem, can you read the code and explain to me what the code does?\n",
    "\n",
    "***paste of the monte hall code in the tutorial notebook***\n",
    "\n",
    "**ChatGPT:** This Python code simulates the Monty Hall problem using a strategy where you always switch your initial door choice after a non-winning (goat) door is revealed. Here’s an explanation of what each part of the code does:\n",
    "\n",
    "### 1. Initial setup\n",
    "```python\n",
    "import numpy as np\n",
    "all_door_options = (1, 2, 3)  # tuple\n",
    "my_door_choice = 1  # Initial choice of door, arbitrarily set to 1\n",
    "i_won = 0  # Variable to count how many times the player wins\n",
    "reps = 10000  # Number of simulations\n",
    "```\n",
    "- `all_door_options`: Represents the 3 doors in the game (1, 2, and 3).\n",
    "- `my_door_choice`: The player's initial choice of door (always door 1 in this simulation).\n",
    "- `i_won`: Tracks how many times the player wins by switching.\n",
    "- `reps`: Number of repetitions or simulations of the Monty Hall problem (10,000).\n",
    "\n",
    "### 2. Simulation loop\n",
    "```python\n",
    "for i in range(reps):\n",
    "    secret_winning_door = np.random.choice(all_door_options)\n",
    "```\n",
    "- A loop is run 10,000 times, simulating the game each time.\n",
    "- `secret_winning_door`: The door hiding the prize is chosen randomly from the three doors.\n",
    "\n",
    "### 3. Determine which door to reveal (goat door)\n",
    "```python\n",
    "    all_door_options_list = list(all_door_options)\n",
    "    all_door_options_list.remove(secret_winning_door)\n",
    "    try:\n",
    "        all_door_options_list.remove(my_door_choice)\n",
    "    except:\n",
    "        pass\n",
    "```\n",
    "- The code creates a list version of `all_door_options` so that doors can be removed.\n",
    "- The door that hides the prize is removed from `all_door_options_list`.\n",
    "- It tries to remove the player's initially chosen door. If it has already been removed (i.e., if the player’s initial choice is the winning door), the `try-except` block prevents an error.\n",
    "\n",
    "### 4. Reveal a goat door\n",
    "```python\n",
    "    goat_door_reveal = np.random.choice(all_door_options_list)\n",
    "    all_door_options_list.remove(goat_door_reveal)\n",
    "```\n",
    "- One of the remaining non-winning doors (a \"goat\" door) is randomly revealed and removed from the list.\n",
    "\n",
    "### 5. Switching strategy\n",
    "```python\n",
    "    if secret_winning_door != my_door_choice:\n",
    "        all_door_options_list.append(secret_winning_door)\n",
    "    my_door_choice = all_door_options_list[0]\n",
    "```\n",
    "- If the player's initial choice is not the winning door, the winning door is added back to the list.\n",
    "- The player's new choice becomes the remaining door in `all_door_options_list`, which will either be the winning door or the last remaining goat door.\n",
    "\n",
    "### 6. Check if the player won\n",
    "```python\n",
    "    if my_door_choice == secret_winning_door:\n",
    "        i_won += 1\n",
    "```\n",
    "- If the player’s new choice (after switching) matches the `secret_winning_door`, the player wins, and `i_won` is incremented.\n",
    "\n",
    "### 7. Calculate win rate\n",
    "```python\n",
    "i_won / reps\n",
    "```\n",
    "- The code calculates the win rate by dividing the number of wins (`i_won`) by the total number of simulations (`reps`).\n",
    "\n",
    "### Summary\n",
    "The code runs a Monte Hall simulation where the player always switches their door after a non-winning door is revealed. The expected result is that the player wins about 2/3 of the time, which is the theoretical probability when always switching.\n",
    "\n",
    "---\n",
    "\n",
    "The response from ChatGPT was able to break down the simulation code to 7 different sections, each corresponding to one stage of the Monte Hall Problem.\n",
    "\n",
    "First, the door options are listed out and the player chooses an arbitrary door number to pick, in this case, door 1 is always chosen. Then a for loop is initiated, repeating the simulation 10,000 times. There is then a new list that includes all the remaining door choices, the one \"goat\" door is revealed and thus removed from the choices, the door that the player initially chose is also thus removed (or the winning door is removed, but in the end there should only be one door remaining), leaving one option left in `all_door_options_list`, then the door number in `all_door_options_list` is checked if it is the same as the number the player chose, if the number is the same, `i_won` is incremented by one.\n",
    "\n",
    "This version of the simulation of the Monte Hall problem is quite interesting because it full simulates the Monte Hall problem process, and like ChatGPT outlines, in 7 different stages. However, as we will later see, this is in fact not quite efficient... And there is a much better way to implement this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "---\n",
    "\n",
    "### Conversation with ChatGPT:\n",
    "\n",
    "**Me:** I see, can you suggest a more simple and streamlined way to implement this? It should also be more clear and easier to understand\n",
    "\n",
    "**ChatGPT:** Certainly! Here's a more streamlined and clearer implementation of the Monty Hall simulation. I've removed some unnecessary steps and comments to make it more readable:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Number of simulations\n",
    "reps = 10000\n",
    "\n",
    "# Track the number of wins\n",
    "wins = 0\n",
    "\n",
    "for _ in range(reps):\n",
    "    # Randomly choose the winning door and the player's initial door\n",
    "    winning_door = np.random.choice([1, 2, 3])\n",
    "    player_choice = np.random.choice([1, 2, 3])\n",
    "    \n",
    "    # If player initially chose the winning door, switching means losing\n",
    "    # If player initially chose a losing door, switching means winning\n",
    "    if player_choice != winning_door:\n",
    "        wins += 1  # Win after switching\n",
    "\n",
    "# Calculate the win percentage when switching\n",
    "win_percentage = wins / reps\n",
    "print(f\"Win percentage when switching: {win_percentage:.2%}\")\n",
    "```\n",
    "\n",
    "### Key Differences:\n",
    "1. **Simplification**:\n",
    "   - There's no need to explicitly \"reveal\" a goat door because, in the switching strategy, we know the player will switch to the door they didn't pick.\n",
    "   - The logic is simplified by directly checking if the player's initial choice was correct or not.\n",
    "\n",
    "2. **Clarity**:\n",
    "   - The simulation focuses on the main point: if the player switches, they win if they initially picked a losing door.\n",
    "   - Fewer variables and cleaner structure make it easier to understand.\n",
    "\n",
    "### Explanation:\n",
    "- For each repetition, the code randomly assigns the winning door and the player's initial choice.\n",
    "- If the player’s initial choice is **not** the winning door, they win by switching (since switching always lands them on the only other door).\n",
    "- After running 10,000 simulations, it calculates the win percentage when following the switch strategy, which should approach 66.67%.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "The response from ChatGPT already included code comments that explained why the code was simplified this way. But I will still try to explain it in my own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage when switching: 66.63%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of simulations\n",
    "reps = 10000\n",
    "\n",
    "# Track the number of wins\n",
    "wins = 0\n",
    "\n",
    "for _ in range(reps):\n",
    "    # Randomly choose the winning door and the player's initial door\n",
    "    winning_door = np.random.choice([1, 2, 3])\n",
    "    player_choice = np.random.choice([1, 2, 3])\n",
    "    \n",
    "    # If player initially chose the winning door, switching means losing\n",
    "    # If player initially chose a losing door, switching means winning\n",
    "    if player_choice != winning_door:\n",
    "        wins += 1  # Win after switching\n",
    "\n",
    "# Calculate the win percentage when switching\n",
    "win_percentage = wins / reps\n",
    "print(f\"Win percentage when switching: {win_percentage:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the final result outputted by the code ChatGPT suggested is 66.63%, which is around the expected range thus the simulation is correct.\n",
    "\n",
    "Visually, the code is a lot shorter than the previous version of the simulation code, but what exactly happened?\n",
    "\n",
    "The way that ChatGPT mainly simplified the code was removing the part that simulated the revealing of the \"goat\" door, and directly going with the statement `if player_choice != winning_door`. The simplified code initially sets up the simulation by setting the number of repetitions and the number of wins, and then starting a for loop, which repeats the simulation. Inside the loop, a random number is chosen from `[1, 2, 3]` for both the player's choice and the winning door, and then whether or not the player wins is determined, where the \"goat\" door revealing simulation was removed. Thinking the logic throughly explains why this can be done, since, in the case that the player initially chose the losing door, after the \"goat\" door was revealed, there is only one choice left, which is the winning door, therefore switching doors meant the player would 100% directly win, therefore `wins += 1` if `player_choice` wasn't `winning_door`. And in the case the player initially chose the winning door, obviously then the only choice left would be the losing door, therefore nothing happens to the total `wins`.\n",
    "\n",
    "By skipping the \"goat\" revealing simulation part, not only is the code much more readable and clear, it also improves the efficiency of the code by removing unnecessary processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "---\n",
    "\n",
    "### Conversation with ChatGPT\n",
    "\n",
    "**Me:** Can you explain what a Markov chain is\n",
    "\n",
    "**ChatGPT:** A **Markov chain** is a mathematical system that models a sequence of events, where the probability of each event depends only on the state attained in the previous event. It’s a type of **stochastic process**, which means it involves randomness, and is named after the Russian mathematician Andrey Markov.\n",
    "\n",
    "### Key Concepts of a Markov Chain:\n",
    "1. **States**: These represent the possible conditions the system can be in. For example, if we were modeling weather, the states could be \"sunny,\" \"cloudy,\" and \"rainy.\"\n",
    "   \n",
    "2. **Transition Probabilities**: These are the probabilities of moving from one state to another. In a Markov chain, these probabilities depend solely on the current state and not on the sequence of events that preceded it. This property is called the **Markov property** or **memorylessness**.\n",
    "\n",
    "3. **Transition Matrix**: This is a matrix that lists the probabilities of moving from each state to every other state. If a system has three states (A, B, and C), the transition matrix might look like this:\n",
    "\n",
    "   \\[\n",
    "   \\begin{pmatrix}\n",
    "   P(A \\to A) & P(A \\to B) & P(A \\to C) \\\\\n",
    "   P(B \\to A) & P(B \\to B) & P(B \\to C) \\\\\n",
    "   P(C \\to A) & P(C \\to B) & P(C \\to C) \\\\\n",
    "   \\end{pmatrix}\n",
    "   \\]\n",
    "   Each row represents the state you're coming from, and each column represents the state you're going to.\n",
    "\n",
    "### Example\n",
    "Imagine flipping a coin. You can model the state of the system as either \"Heads\" or \"Tails.\" Each flip depends only on the result of the previous flip, with a 50% chance of either outcome. The transition matrix would be:\n",
    "\n",
    "\\[\n",
    "\\begin{pmatrix}\n",
    "0.5 & 0.5 \\\\\n",
    "0.5 & 0.5 \\\\\n",
    "\\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "### Properties:\n",
    "- **Absorbing States**: Some states might be \"absorbing,\" meaning that once the system enters such a state, it stays there forever.\n",
    "- **Steady-State**: In some Markov chains, after many transitions, the probabilities of being in each state converge to a stable distribution, known as the **steady-state distribution**.\n",
    "\n",
    "Markov chains are widely used in areas like economics, game theory, genetics, and natural language processing (for example, text generation).\n",
    "\n",
    "**Me:** I have a python code that implements a \"Markov ChatBot\", which utilizes markov chains to make a chatbot, like you, can you read the code and suggest how the code works?\n",
    "\n",
    "***Pasted Markovian ChatBot Code***\n",
    "\n",
    "**ChatGPT:** This Python code implements part of a **Markovian chatbot**, which utilizes Markov chains to model the relationship between words and predict the next word based on the previous one. Here’s how the code works:\n",
    "\n",
    "### Step-by-Step Breakdown\n",
    "\n",
    "1. **Initial Data Structures:**\n",
    "   - `word_used`: This dictionary keeps track of how often each word appears in the text.\n",
    "   - `next_word`: This is a dictionary of dictionaries that stores how often each word follows another word. Each word (as a key) will map to another dictionary, which keeps track of subsequent words and their counts.\n",
    "\n",
    "2. **Iterating Through Words:**\n",
    "   - `words[:-1]`: The code is iterating over all words in the list `words`, except the last one (since we're comparing each word to the word that follows it).\n",
    "   - `for i, word in enumerate(words[:-1])`: This loop goes over each word in the sequence `words` by its index and value (`i` and `word`), preparing to record both the word's usage frequency and the next word that follows it.\n",
    "\n",
    "3. **Tracking Word Usage (`word_used`):**\n",
    "   - `if word in word_used`: \n",
    "     - If the word has already appeared, increment its count in `word_used`.\n",
    "     - If it hasn’t appeared before, initialize its count to 1 and create an empty dictionary for tracking which words follow it in `next_word`.\n",
    "\n",
    "4. **Tracking Word Transitions (`next_word`):**\n",
    "   - The code then checks for the next word (`words[i+1]`), which is the word that follows the current word (`word`).\n",
    "   - `if words[i+1] in next_word[word]`: \n",
    "     - If this next word has already followed the current word before, the program increments the count, indicating that this transition has happened again.\n",
    "     - If this next word hasn’t followed the current word before, it's added to the dictionary, and the count is initialized to 1.\n",
    "\n",
    "### Purpose of the Code:\n",
    "- The core idea here is to **build a Markov chain model** for the chatbot by analyzing the training data (the `words` list).\n",
    "- `word_used` keeps track of how frequently each word appears in the training text.\n",
    "- `next_word` models the relationships between words, allowing the chatbot to predict the next word by looking at the most likely next word based on the current one.\n",
    "\n",
    "### How it Works for a Markov Chatbot:\n",
    "1. The chatbot can start with any word.\n",
    "2. Based on this word, it looks up the possible words that might follow it using the `next_word` dictionary.\n",
    "3. It randomly selects one of the most frequent following words.\n",
    "4. The chatbot then repeats this process to construct a sentence or a response.\n",
    "\n",
    "### Suggested Enhancements:\n",
    "1. **Word Initialization**: Consider handling the case where the `words` list is empty or very short to avoid indexing errors (`words[i+1]` would cause an issue if the list is empty or only one word long).\n",
    "   \n",
    "2. **Transition Probabilities**: Instead of counting occurrences, you could normalize the `next_word` values to represent probabilities (i.e., by dividing by the total number of transitions for that word). This would allow you to sample the next word according to its probability.\n",
    "\n",
    "3. **Improving Readability with `defaultdict`**: You have commented out the use of `defaultdict` from the `collections` module, but it's a cleaner and more efficient way to manage default values for dictionaries. You can use it to simplify your code:\n",
    "\n",
    "   ```python\n",
    "   from collections import defaultdict\n",
    "\n",
    "   word_used = defaultdict(int)\n",
    "   next_word = defaultdict(lambda: defaultdict(int))\n",
    "   ```\n",
    "\n",
    "This would automatically initialize dictionary values when needed, eliminating the need for the manual `if-else` checks for initialization.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To my understanding after watching the video and the response from ChatGPT, a Markov chain is mainly about the probability of \"states\" transforming into another \"state\". To place this under the context of words or sentences, it is the probability of one word being followed by another word.\n",
    "\n",
    "With the asistance of the ChatBot, I can also make a breakdown of how the Markovian Chatbot Code works. It essentially records the frequency which a word is followed by another word. There are two dictionaries that are initialized, one of which records the frequency of a word being present, the other records the frequency of a word being followed by another specific word. \n",
    "\n",
    "It is quite straightforward, the first if-else statement adds a word into the dictionary that records the frequency of the word if it's not recorded yet, and adds the word in the dictionary that records the frequency of following words, or adds one to the frequency. The second if-else statement looks for words that follow the word at position `i`, which is `i+1`, and tally these up in the `next_word` dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "---\n",
    "\n",
    "### Conversation with ChatGPT\n",
    "\n",
    "**Me:** Based on the previous implementation of the Markovian ChatBot, I have a few extensions to the previous code, can you tell me what this extension does?\n",
    "\n",
    "***Pasted First Extension***\n",
    "\n",
    "**ChatGPT:** This extension to the original Markovian Chatbot code modifies the model to use **bigrams** (pairs of words) instead of individual words for transitions. This increases the complexity and context-awareness of the chatbot by taking into account **two consecutive words** to predict the next word, instead of just a single word.\n",
    "\n",
    "### Breakdown of the Extension:\n",
    "\n",
    "1. **Data Structures**:\n",
    "   - `word_used2`: This dictionary tracks how often pairs of consecutive words (bigrams) appear in the text.\n",
    "   - `next_word2`: This dictionary of dictionaries now maps a bigram (two consecutive words) to the set of words that can follow that bigram, along with their frequencies.\n",
    "\n",
    "2. **Iterating Through Words**:\n",
    "   - `for i, word in enumerate(words[:-2])`: \n",
    "     - The code now iterates over each word in the list `words`, except the last two (`words[:-2]`), because it needs to look two words ahead.\n",
    "     - `word` represents the current word, and `words[i+1]` is the word following it. Together, these form a bigram.\n",
    "\n",
    "3. **Tracking Bigram Usage (`word_used2`)**:\n",
    "   - `word_used2[word + ' ' + words[i+1]]`: \n",
    "     - Instead of tracking the frequency of individual words, the model now tracks the frequency of **word pairs** (bigrams). The bigram is created by concatenating the current word (`word`) and the next word (`words[i+1]`), separated by a space.\n",
    "     - The frequency of each bigram is incremented every time that bigram appears in the text.\n",
    "\n",
    "4. **Tracking Transitions After Bigrams (`next_word2`)**:\n",
    "   - `next_word2[word + ' ' + words[i+1]][words[i+2]]`: \n",
    "     - This tracks which word (`words[i+2]`) follows the bigram (`word + ' ' + words[i+1]`).\n",
    "     - The code increments the count of how often `words[i+2]` follows the bigram, updating the dictionary accordingly.\n",
    "\n",
    "### Example:\n",
    "Let’s assume `words = ['the', 'cat', 'sat', 'on', 'the', 'mat']`.\n",
    "\n",
    "- In the first iteration (`i = 0`), the bigram is `'the cat'` and the next word is `'sat'`. The code:\n",
    "  - Updates `word_used2['the cat'] += 1` (this bigram appeared once).\n",
    "  - Updates `next_word2['the cat']['sat'] += 1` (the word `'sat'` follows the bigram `'the cat'` once).\n",
    "  \n",
    "- In the second iteration (`i = 1`), the bigram is `'cat sat'` and the next word is `'on'`. The code:\n",
    "  - Updates `word_used2['cat sat'] += 1`.\n",
    "  - Updates `next_word2['cat sat']['on'] += 1`.\n",
    "\n",
    "And so on for each bigram.\n",
    "\n",
    "### Purpose of the Extension:\n",
    "- By using **bigrams** (two consecutive words) rather than single words, the chatbot can generate more coherent and contextually relevant sentences. It has more context when choosing the next word, since it considers the previous two words instead of just one.\n",
    "- This approach is more flexible and produces more natural language than the unigram (single-word) model. For example, in a unigram model, a word like \"the\" could be followed by any number of words, but in a bigram model, the sequence \"the cat\" narrows down the possible next words to something more appropriate, like \"sat\" or \"is.\"\n",
    "\n",
    "### Key Differences from the Previous Code:\n",
    "1. The previous code uses **single words** as the state, whereas this version uses **bigrams** (pairs of words) as the state.\n",
    "2. The chatbot in this version can generate more contextually appropriate responses because it considers two words before choosing the next word.\n",
    "3. The transition matrix effectively becomes more detailed, as it tracks the likelihood of a word following specific bigrams rather than just individual words.\n",
    "\n",
    "This extension makes the Markovian chatbot more context-aware, resulting in more fluent and meaningful responses.\n",
    "\n",
    "\n",
    "**Me:** Here is another extension, also to the Markovian Chatbot implementation earlier, can you tell me what this extension does?\n",
    "\n",
    "***Pasted second extension***\n",
    "\n",
    "**ChatGPT:** This extension further enhances the Markovian Chatbot by associating word transitions with specific **characters** from a dataset (likely from a script or dialogue). The main goal of this extension is to create **character-specific Markov chains**, where the chatbot generates responses that are more consistent with the speech patterns of different characters.\n",
    "\n",
    "### Breakdown of the Extension:\n",
    "\n",
    "1. **Characters Data (`characters` Variable)**:\n",
    "   - The `characters` variable is built using the `Counter` class from the `collections` module, which counts the frequency of different characters from the `avatar.character` column of a dataset. \n",
    "   - The `avatar.character` column is processed by:\n",
    "     - Converting the text to **uppercase**.\n",
    "     - Replacing **spaces with dots** (`' ' -> '.'`), likely to normalize the data format for easier processing.\n",
    "   - This results in a dictionary (`characters`) where keys are characters (in uppercase and with dots instead of spaces) and values are their frequency in the dataset.\n",
    "\n",
    "2. **Nested Dictionary (`nested_dict`)**:\n",
    "   - The `nested_dict` is defined using a `lambda` function that recursively creates a `defaultdict` of `defaultdicts`. This allows for the creation of arbitrarily nested dictionaries, which are useful for storing complex structures like word transitions for different characters.\n",
    "   - `word_used2C` and `next_word2C` use this nested dictionary structure to store bigram transitions specific to each character.\n",
    "\n",
    "3. **Iterating Over Words**:\n",
    "   - The loop (`for i, word in enumerate(words[:-2])`) iterates over the `words` list, similar to the previous extension, using word bigrams to track transitions.\n",
    "   - **Character Identification**: \n",
    "     - For each word, the code checks if the word is in the `characters` dictionary (which means it corresponds to a character's name). \n",
    "     - If the word is a character, it assigns the `character` variable to that word, which will now be used to associate bigrams with this character.\n",
    "   \n",
    "4. **Tracking Bigrams for Each Character (`word_used2C`)**:\n",
    "   - For each character, the code keeps track of the frequency of bigrams in their speech:\n",
    "     - `word_used2C[character][word+' '+words[i+1]]`: This stores how often a particular bigram appears in the speech of the identified `character`.\n",
    "     - If the bigram is new for this character, it's initialized to 0, then incremented by 1 whenever the bigram is encountered.\n",
    "\n",
    "5. **Tracking Word Transitions After Bigrams for Each Character (`next_word2C`)**:\n",
    "   - Similarly, for each character, the code tracks which word follows a given bigram:\n",
    "     - `next_word2C[character][word+' '+words[i+1]][words[i+2]]`: This stores how often a specific word (`words[i+2]`) follows a particular bigram (`word + ' ' + words[i+1]`) in the speech of the identified `character`.\n",
    "     - If this word hasn't followed the bigram before, it's initialized to 0, then incremented each time this transition occurs.\n",
    "\n",
    "### Example:\n",
    "Let’s assume the `words` list contains dialogue, and the dataset `avatar.character` contains character names like \"AANG\" and \"KATARA\". If the chatbot encounters the word \"AANG\", it switches to tracking bigrams and transitions for **Aang’s speech patterns**.\n",
    "\n",
    "- Suppose the words are `[\"AANG\", \"THE\", \"WIND\", \"IS\", \"STRONG\"]`.\n",
    "  - When it encounters \"AANG\", the `character` is set to \"AANG\".\n",
    "  - The bigram `'THE WIND'` is tracked for Aang, and the word `'IS'` is recorded as following this bigram.\n",
    "  \n",
    "- For `next_word2C['AANG']['THE WIND']['IS']`, the frequency of this transition is incremented.\n",
    "\n",
    "### Purpose of the Extension:\n",
    "- **Character-Specific Markov Chains**: This extension makes the Markov model **character-aware**, meaning that each character gets its own unique set of word transitions based on their speech patterns. This is useful when modeling dialogue for multiple characters with distinct speaking styles.\n",
    "- **Dynamic Character Switching**: The model can switch between characters dynamically based on the words it encounters in the text (when the word matches a character's name).\n",
    "- **Improved Response Generation**: When generating responses, the chatbot can now respond in a way that mimics the speech patterns of different characters from the dataset, allowing for more natural and varied dialogue.\n",
    "\n",
    "### Key Differences from Previous Versions:\n",
    "1. **Character Association**: The most significant addition is that the chatbot now tracks word transitions separately for each character, rather than treating the text as a single, undifferentiated body of words.\n",
    "2. **Nested Dictionaries**: The use of `nested_dict` allows for character-specific tracking of bigrams and the words that follow them.\n",
    "3. **Dynamic Context**: The chatbot switches context based on the character it encounters, so the generated responses can be tailored to mimic how a specific character speaks.\n",
    "\n",
    "### Use Case:\n",
    "This extension is particularly useful in scenarios like chatbots for dialogue systems in games, movies, or TV shows, where different characters have distinct vocabularies or speaking styles. The chatbot can generate responses that are character-specific, making it more immersive and believable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.1\n",
    "\n",
    "\n",
    "\n",
    "## Question 6.2\n",
    "\n",
    "This is mainly the reason why I personally do not really enjoy talking to ChatBots when I'm trying to figure out something, sometimes they can be very unhelpful, being unable to efficiently explain something. For example, sometimes the responses ChatBots produce can be too wordy, not cutting straight to the point, and require more reading. ChatBots aren't humans, they do not interact with us the same way we would to another human, thus the interactions with a ChatBot could not feel natural and therefore take more effort. \n",
    "\n",
    "Secondly, the nature of these ChatBots, their ability to \"chat\" are somewhat restrictive, as their only form of output is words and text, whereas using websites to do research on topics could benefit from the freedom of usage of images and graphics. \n",
    "\n",
    "Somtimes it takes time for ChatBots to produce results that are useful, as I have experienced one during the previous homework (HW 01) where ChatGPT would keep on producing responses that were way too out of the scope of what I was looking for, often being overly in depth or overly complicated, even after multiple tries and new Chat sessions. It's just very difficult to talk to someone who simply isn't another human being, the way we interact is very different, and it's sometimes just difficult to make \"it\" understand me.\n",
    "\n",
    "## Question 6.3\n",
    "\n",
    "Personally as a person that has considerable amounts of coding experience and has always been more inclined towards not using ChatBots, I find myself having a new understanding of what ChatBots can assist me in doing, but still are somewhat restricted. \n",
    "\n",
    "In my experience in recent interactions with ChatBots, particularly the interactions in the preivous homework (HW 01), where I had to intentionally produce errors and see of the ChatBot can determine what went wrong, I think this is where ChatBots can be quite useful. Since their ability to include **context** in their responses can be very beneficial and help produce useful responses, such as understanding and explaining what went wrong, and most importantly suggest a fix for the error.\n",
    "\n",
    "I would also say that ChatBots coul also be quite useful when looking for assistance for understanding code. Such as the second Markovian ChatBot extension, which, although is a good implementation, is still quite a jumble of mess, not properly organized, not commented enough to explain what each line of the code does, and is just simply confusing to read **(which is very bad coding habits!!!)** ~~(I know it's done for the sake of the assignment to use the ChatBot to help me understand but still...)~~. Using ChatBots can help to break down code part by part to help understand what each section or even each line of the code does. \n",
    "\n",
    "Which, I would like to add and link to the previous homework again, would not be possible when doing searches online such as Google, since you can't paste this huge code block into Google and expect there to be some result magically explaining what the code does. Maybe posting it on StackOverflow can help but it's weird posting a question of StackOverflow and asking what a code block does...\n",
    "\n",
    "HOWEVER, in my experience of using ChatBots so far, I would still be inclined towards using online searches when looking for answers and explanations for certain topics, such as \"What is a Markov Chain\" (which I think the video did a very good job at explaining compared to the wordy explanation produced by Chat), or when asking for Chat to generate code for me, because it's just hard to fully communicate my needs to a non-human, and often times, like I've experienced in the previous homework, I just can't get the results I wanted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
