{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA130 Homework 08\n",
    "\n",
    "*So... We have arrived... The final level...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to ChatGPT Chat Logs:\n",
    "\n",
    "Question 1 and 2 - Introduction to CDTs:\n",
    "\n",
    "\n",
    "\n",
    "Question 3 - EDAs:\n",
    "\n",
    "\n",
    "\n",
    "Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### What are CDTs? What Problems do they address?\n",
    "\n",
    "Classification Decision Trees (CDTs) are a non-parametric, hierarchial model that attemps to split an original group of predictors into smaller groups and maximizes homogeneity within each group, with each group represented as some categorical outcome variable. CDTs attempt to do this by recursively applying decision rules (sometimes called \"nodes\") that split the predictor entities into one subset or another, making each subset at each \"node\" as distinct as possible in terms of the target outcome variables.\n",
    "\n",
    "CDTs are non-parametric, meaning that the predictors and outcome variables do not need to take some \"functional\" form, such as linearity, quadratic, etc. This makes CDTs much more flexible in terms of what can be inputted as predictors as well as outcome variables, and be more robust when predictors have interactions between them. (We've seen in previous HWs that interactions can be quite complex and bothersome)\n",
    "\n",
    "In short, CDTs attempt to partition the data into \"regions\", with each region containing discrete predictor \"items\". Each \"region\" corresponds to a \"leaf node\" of the CDT, each represeting one of the outcome (target) variables. The way these regions are partitioned aims to minimize the \"impurity\" of the region, where higher \"purity\" meaning more correctly categorized predictors in each region.\n",
    "\n",
    "CDTs are designed to and are mainly used to solve classification problems, to categorize predictors (objects) into categories (or more technically called \"classes\") based on the characteristics of the predictor.\n",
    "\n",
    "In the real world, CDTs are widely used in medical related fields, such as medical diagnosis. A CDT can be used to identify whether or not a patient has a certain disease or not based on the characteristics of the patiet, such as symptoms, lab results, or even imaging results. In the modelling *(might be more commonly called \"training\" in the field of machine learning)* process (which we will demonstrate later), the algorithm chooses which characteristics are most useful in distinguishing between one class or another, and chooses the specific value(s) that separate the inputs into the classes.\n",
    "\n",
    "For example, the root (top/first) node might separate patients into two groups based on whether or not they have a fever, then in subsequent nodes, the groups are further split based on other (maybe repeating) characteristics, such as, \"blood pressure > 140mmHg\" into one group, and \"blood pressure <= 140mmHg\" into another group. This process is continued until all patients have been categorized into one group or another, \"has the disease\" or \"does not have the disease\". Then, when we want to use the CDT to categorize a new patient, we simply follow the tree down the path with the patient's features.\n",
    "\n",
    "### CDTs vs MLR models\n",
    "\n",
    "We've previously (in other HWs) looked at Multiple Linear Regression (MLR), and seen how MLRs take in predictors and come up with a prediction of the outcome based on the predictor. We've also seen how MLR models can use both categorical data and numerical data as predictors and also output both types of data. This is similar to a CDT, where both types of data can be used to predict both types of data, but CDTs and MLR models are fundamentally different. CDTs go through the process of classification to come up with outcomes, while MLRs go through the process of regression.\n",
    "\n",
    "For regression, we are finding best values of **coefficients** to fit a **function** to a set of predictors, the final form of a regression model is a **function** (a **linear function** for MLR). This is fundamentally different compared to classification, where **partition (classification) rules** are defined to **partition** a predictor space (a group of predictors) into hierarchial regions, and the final result of a classification model is a **tree** like structure.\n",
    "\n",
    "For MLR, when making a prediction, we input values for predictors, and calculate the final result by multiplying predictor variables with the coefficients. For CDTs, we make predictions by following the rules specified by each node in the tree, and follow the path down until the leaf nodes are reached, where a predictor is finally classified into a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Before we start, I want to first start by identifying what $TP$, $TN$, $FP$, $FN$ are:\n",
    "\n",
    "$TP$: True Positive, when the prediction is ****positive****, and the real outcome is also ****positive****.\n",
    "\n",
    "$TN$: True Negative, when the prediction is **negative**, and the real outcome is also **negative**.\n",
    "\n",
    "$FP$: False Positive, when the prediction is **positive**, but the real outcome is **negative**.\n",
    "\n",
    "$FN$: False Negative, when the prediction is **negative**, but the real outcome is **positive**.\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy measures the overall correctness of the model. It is the propotion of total predictions that the model got correct (both positives and negatives), it is calculated by:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "A real word example could be spam email filters. For a spam email filtering model, a true positve is when a spam email is correctly identified as a spam, and a true negative is when non-spam email is correctly identified as non-spam.\n",
    "\n",
    "We value accuracy the most in this example the most because we care about the accuracy of both positive and negatives equally. It is equally important that spam email isn't identified as non-spam, as we really just don't want spam email, and that non-spam email isn't identified as spam, as we might not be able to see important email in time.\n",
    "\n",
    "*This may not seen very obvious as of now, but examples for other metrics later we will talk about will show the difference.*\n",
    "\n",
    "### Sensitivity\n",
    "\n",
    "Sensitivity measures how well the model identifies positive cases. It is the proportion of actual positives that the model got correct, it is calculated by:\n",
    "\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "A real world example could be cancer detection, where a true positive is when a patient that has cancer is correctly identified as a cancer patient, and a false negative is when a patient that doesn't have cancer is falsely identified as a cancer patient.\n",
    "\n",
    "We value sensitivity in this example the most because we care about the model being able to correctly identify patients the most. Cancer patients are in urgent need of treatment, otherwise they may face severe deline in health and even death. Therefore correctly identifying patients is the utmost important thing for such a model, while the other cases, such as false positives, when falsely identifying a patient as a cancer patient, have much less consequences and/or have the least affect on stakeholders of the model.\n",
    "\n",
    "### Specificity\n",
    "\n",
    "Specificity measures how well the model identifies negative cases. It is the proportion of actual negatives that the model got correct, it is calculated by:\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "A real world example could be fraud detection, where a true negative is when a transaction that is non-fraudulent is correctly classified as non-fraudulent, and a false positive is when a non-fraudulent transaction is falsely identified as fraudulent. When true negatives are high, leading to higher specificity, the number of false negatives \n",
    "\n",
    "We calue specificity the most in this example when we care the most about ensuring that the customers being able to conveniently complete transactions. Therefore, we value the true negative rate the most, when true negatives are high, it means that most of the non-fraudulent transactions are correctly identified, and the transaction is completed for the customer without obstruction.\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision measures accuracy of positive predictions. It is the proportion of positive predictions that are actually correct, it is calculated by:\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "A real world example could be predicting crime risk of individuals, where a true positive is when a person is correctly identified as likely to commit a crime(s), and a false positive is when a person is wrongly identified as likely to commit a crime(s).\n",
    "\n",
    "We value precision in this example the most because we want to ensure that those who are actually going to commit crimes are identified and flagged (possibly put under surveilance), so that they do not continuously pose a risk to society by remaining not flagged. As the same for other cases, the consequences of not being able to identify a person with high crime risks are far higher than other outcomes, such as falsely flagging a person with low crime risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "I wasn't sure what EDAs were, so I went for a quick search.\n",
    "\n",
    "EDAs aim to summarize, visualize and attempt to understand data to uncover patterns, identify anomalies, and inform further analysis.\n",
    "\n",
    "In the following code I attempt to pre-process the data to meet the requirements and briefly provide initial EDAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>List Price</th>\n",
       "      <th>Amazon Price</th>\n",
       "      <th>Hard_or_Paper</th>\n",
       "      <th>NumPages</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Pub year</th>\n",
       "      <th>ISBN-10</th>\n",
       "      <th>Thick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,001 Facts that Will Scare the S#*t Out of Yo...</td>\n",
       "      <td>Cary McNeal</td>\n",
       "      <td>12.95</td>\n",
       "      <td>5.18</td>\n",
       "      <td>P</td>\n",
       "      <td>304</td>\n",
       "      <td>Adams Media</td>\n",
       "      <td>2010</td>\n",
       "      <td>1605506249</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21: Bringing Down the House - Movie Tie-In: Th...</td>\n",
       "      <td>Ben Mezrich</td>\n",
       "      <td>15.00</td>\n",
       "      <td>10.20</td>\n",
       "      <td>P</td>\n",
       "      <td>273</td>\n",
       "      <td>Free Press</td>\n",
       "      <td>2008</td>\n",
       "      <td>1416564195</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100 Best-Loved Poems (Dover Thrift Editions)</td>\n",
       "      <td>Smith</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>P</td>\n",
       "      <td>96</td>\n",
       "      <td>Dover Publications</td>\n",
       "      <td>1995</td>\n",
       "      <td>486285537</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1421: The Year China Discovered America</td>\n",
       "      <td>Gavin Menzies</td>\n",
       "      <td>15.99</td>\n",
       "      <td>10.87</td>\n",
       "      <td>P</td>\n",
       "      <td>672</td>\n",
       "      <td>Harper Perennial</td>\n",
       "      <td>2008</td>\n",
       "      <td>61564893</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1493: Uncovering the New World Columbus Created</td>\n",
       "      <td>Charles C. Mann</td>\n",
       "      <td>30.50</td>\n",
       "      <td>16.77</td>\n",
       "      <td>P</td>\n",
       "      <td>720</td>\n",
       "      <td>Knopf</td>\n",
       "      <td>2011</td>\n",
       "      <td>307265722</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Where the Sidewalk Ends</td>\n",
       "      <td>Shel Silverstein</td>\n",
       "      <td>18.99</td>\n",
       "      <td>12.24</td>\n",
       "      <td>H</td>\n",
       "      <td>192</td>\n",
       "      <td>HarperCollins</td>\n",
       "      <td>2004</td>\n",
       "      <td>60572345</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>White Privilege</td>\n",
       "      <td>Paula S. Rothenberg</td>\n",
       "      <td>27.55</td>\n",
       "      <td>27.55</td>\n",
       "      <td>P</td>\n",
       "      <td>160</td>\n",
       "      <td>Worth Publishers</td>\n",
       "      <td>2011</td>\n",
       "      <td>1429233443</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Why I wore lipstick</td>\n",
       "      <td>Geralyn Lucas</td>\n",
       "      <td>12.95</td>\n",
       "      <td>5.18</td>\n",
       "      <td>P</td>\n",
       "      <td>224</td>\n",
       "      <td>St Martin's Griffin</td>\n",
       "      <td>2005</td>\n",
       "      <td>031233446X</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Worlds Together, Worlds Apart: A History of th...</td>\n",
       "      <td>Robert Tignor</td>\n",
       "      <td>97.50</td>\n",
       "      <td>97.50</td>\n",
       "      <td>P</td>\n",
       "      <td>480</td>\n",
       "      <td>W. W. Norton &amp; Company</td>\n",
       "      <td>2010</td>\n",
       "      <td>393934942</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Wuthering Heights</td>\n",
       "      <td>Emily Bronte</td>\n",
       "      <td>16.99</td>\n",
       "      <td>4.95</td>\n",
       "      <td>P</td>\n",
       "      <td>344</td>\n",
       "      <td>CreatSpace</td>\n",
       "      <td>2011</td>\n",
       "      <td>1463533411</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title               Author  \\\n",
       "0    1,001 Facts that Will Scare the S#*t Out of Yo...          Cary McNeal   \n",
       "1    21: Bringing Down the House - Movie Tie-In: Th...          Ben Mezrich   \n",
       "2         100 Best-Loved Poems (Dover Thrift Editions)                Smith   \n",
       "3              1421: The Year China Discovered America        Gavin Menzies   \n",
       "4      1493: Uncovering the New World Columbus Created      Charles C. Mann   \n",
       "..                                                 ...                  ...   \n",
       "314                            Where the Sidewalk Ends     Shel Silverstein   \n",
       "315                                    White Privilege  Paula S. Rothenberg   \n",
       "316                                Why I wore lipstick        Geralyn Lucas   \n",
       "317  Worlds Together, Worlds Apart: A History of th...        Robert Tignor   \n",
       "318                                  Wuthering Heights         Emily Bronte   \n",
       "\n",
       "     List Price  Amazon Price Hard_or_Paper  NumPages               Publisher  \\\n",
       "0         12.95          5.18             P       304             Adams Media   \n",
       "1         15.00         10.20             P       273              Free Press   \n",
       "2          1.50          1.50             P        96      Dover Publications   \n",
       "3         15.99         10.87             P       672        Harper Perennial   \n",
       "4         30.50         16.77             P       720                   Knopf   \n",
       "..          ...           ...           ...       ...                     ...   \n",
       "314       18.99         12.24             H       192           HarperCollins   \n",
       "315       27.55         27.55             P       160        Worth Publishers   \n",
       "316       12.95          5.18             P       224     St Martin's Griffin   \n",
       "317       97.50         97.50             P       480  W. W. Norton & Company   \n",
       "318       16.99          4.95             P       344              CreatSpace   \n",
       "\n",
       "     Pub year     ISBN-10  Thick  \n",
       "0        2010  1605506249    0.8  \n",
       "1        2008  1416564195    0.7  \n",
       "2        1995   486285537    0.3  \n",
       "3        2008    61564893    1.6  \n",
       "4        2011   307265722    1.4  \n",
       "..        ...         ...    ...  \n",
       "314      2004    60572345    1.1  \n",
       "315      2011  1429233443    0.7  \n",
       "316      2005  031233446X    0.7  \n",
       "317      2010   393934942    0.9  \n",
       "318      2011  1463533411    1.0  \n",
       "\n",
       "[319 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "import graphviz as gv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Drop unwanted columns and rows\n",
    "unwanted_columns = ['Weight_oz', 'Width', 'Height']\n",
    "ab_reduced = ab.drop(unwanted_columns, axis=1)\n",
    "ab_reduced_noNaN = ab_reduced.dropna(ignore_index=True)\n",
    "\n",
    "# Specify column and type to convert to\n",
    "convert_types = {'Pub year': 'int',\n",
    "                 'NumPages': 'int',\n",
    "                 'Hard_or_Paper': 'category'}\n",
    "\n",
    "# Set type\n",
    "ab_reduced_noNaN = ab_reduced_noNaN.astype(convert_types)\n",
    "ab_reduced_noNaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_reduced_noNaN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>List Price</th>\n",
       "      <th>Amazon Price</th>\n",
       "      <th>NumPages</th>\n",
       "      <th>Pub year</th>\n",
       "      <th>Thick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18.362978</td>\n",
       "      <td>12.941034</td>\n",
       "      <td>334.272727</td>\n",
       "      <td>2002.175549</td>\n",
       "      <td>0.903448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.976755</td>\n",
       "      <td>12.436673</td>\n",
       "      <td>161.601510</td>\n",
       "      <td>10.646133</td>\n",
       "      <td>0.365261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1936.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.890000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>1998.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.360000</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>139.950000</td>\n",
       "      <td>139.950000</td>\n",
       "      <td>896.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>2.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       List Price  Amazon Price    NumPages     Pub year       Thick\n",
       "count  319.000000    319.000000  319.000000   319.000000  319.000000\n",
       "mean    18.362978     12.941034  334.272727  2002.175549    0.903448\n",
       "std     13.976755     12.436673  161.601510    10.646133    0.365261\n",
       "min      1.500000      0.770000   24.000000  1936.000000    0.100000\n",
       "25%     13.890000      8.600000  208.000000  1998.000000    0.600000\n",
       "50%     15.000000     10.200000  320.000000  2005.000000    0.900000\n",
       "75%     19.360000     12.560000  416.000000  2010.000000    1.100000\n",
       "max    139.950000    139.950000  896.000000  2011.000000    2.100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_reduced_noNaN.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>ISBN-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>309</td>\n",
       "      <td>251</td>\n",
       "      <td>158</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>The Great Gatsby</td>\n",
       "      <td>Jodi Picoult</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>743273567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Title        Author Publisher    ISBN-10\n",
       "count                319           319       319        319\n",
       "unique               309           251       158        316\n",
       "top     The Great Gatsby  Jodi Picoult   Vintage  743273567\n",
       "freq                   3             7        37          2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_reduced_noNaN.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe do something more here?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(ab_reduced_noNaN, test_size=0.2)\n",
    "ab_reduced_noNaN_train.shape[0] # row count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_reduced_noNaN_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset has 255 observations, and the testing dataset has 64 observations.\n",
    "\n",
    "For the following two lines of code:\n",
    "\n",
    "```python\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "```\n",
    "\n",
    "The first line creates dummy/indicator variable for the `Hard_or_Paper` column, where each unique category in the column becomes a separate column, one for Hardcover, `H`, and one for Paperback, `P`. The rows that are `H` will have a 1 in the `H` column, and 0 in others, same for the `P` column. This new dataframe is `y`, the outcome variable.\n",
    "\n",
    "The second line takes a column from the `ab_reduced_noNaN` dataframe and creates a new dataframe with one column. This new dataframe is `x`, the predicator variable.\n",
    "\n",
    "The two lines define what data are used as the predictor, and what data are used for the outcome (target) classes.\n",
    "\n",
    "Following is the code for \"training\" the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree depth: 2\n",
      "Number of leaves: 4\n"
     ]
    }
   ],
   "source": [
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "\n",
    "# Create a DecisionTreeClassifier with max depth of 2\n",
    "clf = tree.DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Fit the model to the data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Print the trained tree's properties (optional, for verification)\n",
    "print(f\"Tree depth: {clf.get_depth()}\")\n",
    "print(f\"Number of leaves: {clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the results with a visualization of the resulting tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.0 (20241103.1931)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"498pt\" height=\"269pt\"\n",
       " viewBox=\"0.00 0.00 498.25 269.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 265)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-265 494.25,-265 494.25,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"308.75,-261 192.25,-261 192.25,-193 308.75,-193 308.75,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-243.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x[0] &lt;= 17.97</text>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-228.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.394</text>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-213.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 319</text>\n",
       "<text text-anchor=\"middle\" x=\"250.5\" y=\"-198.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [233, 86]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"243.75,-157 127.25,-157 127.25,-89 243.75,-89 243.75,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.5\" y=\"-139.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x[0] &lt;= 10.8</text>\n",
       "<text text-anchor=\"middle\" x=\"185.5\" y=\"-124.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.194</text>\n",
       "<text text-anchor=\"middle\" x=\"185.5\" y=\"-109.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 221</text>\n",
       "<text text-anchor=\"middle\" x=\"185.5\" y=\"-94.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [197, 24]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M229.22,-192.6C224,-184.41 218.34,-175.53 212.88,-166.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.97,-165.3 207.64,-158.75 210.07,-169.06 215.97,-165.3\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.42\" y=\"-176.08\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"371,-157 262,-157 262,-89 371,-89 371,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.5\" y=\"-139.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">x[0] &lt;= 29.225</text>\n",
       "<text text-anchor=\"middle\" x=\"316.5\" y=\"-124.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.465</text>\n",
       "<text text-anchor=\"middle\" x=\"316.5\" y=\"-109.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 98</text>\n",
       "<text text-anchor=\"middle\" x=\"316.5\" y=\"-94.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [36, 62]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>0&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M272.11,-192.6C277.41,-184.41 283.16,-175.53 288.7,-166.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.52,-169.04 294.02,-158.74 285.65,-165.24 291.52,-169.04\"/>\n",
       "<text text-anchor=\"middle\" x=\"300.08\" y=\"-176.12\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"109,-53 0,-53 0,0 109,0 109,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"54.5\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.355</text>\n",
       "<text text-anchor=\"middle\" x=\"54.5\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 52</text>\n",
       "<text text-anchor=\"middle\" x=\"54.5\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [40, 12]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.31,-88.68C126.39,-79.36 112.37,-69.24 99.52,-59.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.78,-57.29 91.62,-54.28 97.69,-62.97 101.78,-57.29\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"243.75,-53 127.25,-53 127.25,0 243.75,0 243.75,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"185.5\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.132</text>\n",
       "<text text-anchor=\"middle\" x=\"185.5\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 169</text>\n",
       "<text text-anchor=\"middle\" x=\"185.5\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [157, 12]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.5,-88.68C185.5,-80.99 185.5,-72.76 185.5,-64.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189,-64.91 185.5,-54.91 182,-64.91 189,-64.91\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"371,-53 262,-53 262,0 371,0 371,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.5\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.382</text>\n",
       "<text text-anchor=\"middle\" x=\"316.5\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n",
       "<text text-anchor=\"middle\" x=\"316.5\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [19, 55]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M316.5,-88.68C316.5,-80.99 316.5,-72.76 316.5,-64.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"320,-64.91 316.5,-54.91 313,-64.91 320,-64.91\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"490.25,-53 388.75,-53 388.75,0 490.25,0 490.25,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"439.5\" y=\"-35.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.413</text>\n",
       "<text text-anchor=\"middle\" x=\"439.5\" y=\"-20.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 24</text>\n",
       "<text text-anchor=\"middle\" x=\"439.5\" y=\"-5.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [17, 7]</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M359.87,-88.68C371.88,-79.45 384.91,-69.44 396.88,-60.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"398.8,-63.18 404.6,-54.32 394.53,-57.63 398.8,-63.18\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x128794290>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = gv.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Yes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
